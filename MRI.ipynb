{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MRI1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushantColumbia/training/blob/main/MRI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFyD2naJEFnd"
      },
      "source": [
        "# Magnetic Resonance Imaging (MRI) I (Sparsity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq2STbU7ESGb"
      },
      "source": [
        "## Looking Ahead\n",
        "\n",
        "In the lecture and in the homework handout, we discussed how signal acquisition in an MRI system leads to a (potentially underdetermined) system of linear equations relating the observations to the unknown target signal (e.g., an image of a cross section of part of a patient's body). \n",
        "\n",
        "This notebook will explore another key aspect of this acquisition scenario: *in a certain basis, the target signal is sparse*. This will allow us, in later notebooks, to apply methods from compressed sensing to the recovery of these data from undersampled measurements.\n",
        "\n",
        "As we saw in the homework writeup and in the lecture, MRI data are acquired in the frequency domain. To analyze the corresponding spatial profile (and its transform-domain sparsity), we will need to perform some signal processing after we download the data. We are using anatomical data from the BOLD5000 dataset: a functional MRI dataset consisting of subjects' brain scans taken while they are observing various images from the ImageNet, SUN, and COCO datasets. In this notebook, we will not analyze any functional data, though. The data are publicly available; more information can be found at the website [here](https://bold5000.github.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC2hiW4G4U5p"
      },
      "source": [
        "## Getting the Data and Setting Up\n",
        "\n",
        "We use the AWS S3 protocol to download a certain subset of the data, corresponding to one subject's anatomical data. For more information on the dataset, see [here](https://openneuro.org/datasets/ds001499/versions/1.3.0) and the BOLD5000 paper on the official website linked above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Qs53KAYIC0"
      },
      "source": [
        "### Install AWS Command Line Utilities\n",
        "These utilities let us download the dataset. \n",
        "\n",
        "**Note**: You may need to restart the notebook session after you download awscli below, per a warning message I get from Colab; things may also work fine if you skip the restart, though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co0IxVRF7F2i"
      },
      "source": [
        "## Install AWS CLI tools\n",
        "!pip install awscli\n",
        "## Prepare data directory\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!mkdir bold5000\n",
        "os.chdir('/content/bold5000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ciky0mJYQyE"
      },
      "source": [
        "### Download the Anatomical Data\n",
        "\n",
        "We download the (anonymized) anatomical data from the third subject in the BOLD5000 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oxkq6zF_RTx"
      },
      "source": [
        "## Grab the data\n",
        "#!aws s3 sync --no-sign-request --exclude \"*\" --include \"*06*\" s3://openneuro.org/ds001499/derivatives/fmriprep/sub-CSI3/ses-13/func/ /content/bold5000/sub-CSI3_ses-13_run-06/\n",
        "!aws s3 sync --no-sign-request s3://openneuro.org/ds001499/sub-CSI3/ses-16/anat/ /content/bold5000/sub-CSI3_anat/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEbRK1OgDBdM"
      },
      "source": [
        "### Auxiliary definitions\n",
        "\n",
        "We will use these functions throughout the demo. They consist of a helper function for plotting images in a format similar to MATLAB, and a few wrappers for the wavelet transforms we will use throughout to hide some of the unimportant implementation details. More on the latter below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mJY2-9JwF9a"
      },
      "source": [
        "## Auxiliary code for our wavelet experiments\n",
        "import bokeh\n",
        "import bokeh.plotting as bpl\n",
        "from bokeh.models import ColorBar, BasicTicker, LinearColorMapper\n",
        "import pywt\n",
        "\n",
        "## Try to do something like imagesc in MATLAB using Bokeh tools.\n",
        "def imagesc(M, title=''):\n",
        "  m, n = M.shape\n",
        "  \n",
        "  # 600 px should be good; calculate ph to try to get aspect ratio right\n",
        "  pw = 600\n",
        "  ph = round(1.0 * pw * m / n)\n",
        "  h = bpl.figure(plot_width = pw, plot_height = ph, x_range=(0, 1.0*n),\n",
        "                 y_range=(0, 1.0*m), toolbar_location='below',\n",
        "                 title=title, match_aspect=True\n",
        "                )\n",
        "  \n",
        "  minval = np.min(M)\n",
        "  maxval = np.max(M)\n",
        "  \n",
        "  color_mapper = LinearColorMapper(palette=\"Greys256\", low=minval, high=maxval)\n",
        "  h.image(image=[M], x=0, y=0, dw=1.0*n, dh=1.0*m, color_mapper=color_mapper)\n",
        "  \n",
        "  color_bar = ColorBar(color_mapper=color_mapper, ticker=BasicTicker(),\n",
        "                      label_standoff=12, border_line_color=None, location=(0, 0))\n",
        "  \n",
        "  h.add_layout(color_bar, 'right')\n",
        "  \n",
        "\n",
        "  bpl.show(h)\n",
        "  return h\n",
        "  \n",
        "## Standard basis element function for matrix space (ZERO-INDEXED)\n",
        "def stdbel(n, i, j):\n",
        "  E = np.zeros((n, n))\n",
        "  E[i, j] = 1\n",
        "  return E\n",
        "\n",
        "## Wavelet functions below\n",
        "## Note: we expect all image sizes to be powers-of-two and square!\n",
        "## So if you adapt this code, be sure to fix this or enforce this requirement...\n",
        "\n",
        "# Get a default slice object for a multilevel wavelet transform\n",
        "# Used to abstract this annoying notation out of the transform...\n",
        "def default_slices(levels, n):\n",
        "  c = pywt.wavedec2(np.zeros((n, n)), 'db4', mode='periodization', level=levels)\n",
        "  bye, slices = pywt.coeffs_to_array(c)\n",
        "  return slices\n",
        "\n",
        "# Wrapper for forward discrete wavelet transform\n",
        "# Output data as a matrix (we don't care about tuple format)\n",
        "def dwt(levels, sdom_data):\n",
        "  c = pywt.wavedec2(sdom_data, 'db4', mode='periodization', level=levels)\n",
        "  output, bye = pywt.coeffs_to_array(c)\n",
        "  return output\n",
        "\n",
        "# Wrapper for inverse discrete wavelet transform\n",
        "# Expect wdom_data as a matrix (we don't care about tuple format)\n",
        "def idwt(levels, wdom_data, slices=None):\n",
        "  n = wdom_data.shape[0]\n",
        "  if slices is None:\n",
        "    slices = default_slices(levels, n)\n",
        "  c = pywt.array_to_coeffs(wdom_data, slices, output_format='wavedec2')\n",
        "  return pywt.waverec2(c, 'db4', mode='periodization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4uGuJ_7YZ5m"
      },
      "source": [
        "## Visualizing the Data\n",
        "\n",
        "The MR machine (Siemens 3T Verio) automatically applies inverse Fourier transforms to the measured slice data, so when we perform visualization below it is of the spatial domain data.\n",
        "\n",
        "We then take a look at the data cube we have loaded from several angles. We use the [nibabel](http://nipy.org/nibabel/) MRI data processing library to easily extract the image data from the T1w scan file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzAEA-qpbGzh"
      },
      "source": [
        "## Have a look at the data\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "\n",
        "img = nib.load('/content/bold5000/sub-CSI3_anat/sub-CSI3_ses-16_T1w.nii.gz')\n",
        "\n",
        "data = img.get_fdata()\n",
        "hdr = img.header\n",
        "\n",
        "## Check the data shape and units\n",
        "print(\"Data shape: {}\".format(data.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_74jf0PXbKXd"
      },
      "source": [
        "The dataset consists of a 3D array. Think of the three dimensions as sampled spatial coordinates (x, y, z). How is the coordinate system enforced by the MR scanner oriented? We plot a \"halfway\" slice for each of the three possible axis-aligned plane orientations to find out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upCFvXqTdpSP"
      },
      "source": [
        "## Store dimensions\n",
        "Nx = data.shape[0]\n",
        "Ny = data.shape[1]\n",
        "Nz = data.shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69yq5GyLcxyR"
      },
      "source": [
        "### x slice\n",
        "bpl.output_notebook()\n",
        "\n",
        "imagesc(data[Nx//2, :, :], title='Slicing orthogonal to x-axis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t69iav2IcyA_"
      },
      "source": [
        "### y slice\n",
        "bpl.output_notebook()\n",
        "imagesc(data[:, Ny//2, :], title='Slicing orthogonal to y-axis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkTvimRacyEv"
      },
      "source": [
        "### z slice\n",
        "bpl.output_notebook()\n",
        "imagesc(data[:, :, round(0.6*Nz)], title='Slicing orthogonal to z-axis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyhPnNcvdZ7B"
      },
      "source": [
        "We infer that the y axis is parallel to the subject's \"line of sight\"; the z axis is parallel to the direction of gravity, if the subject were standing upright; and the x axis occupies the remaining orthogonal direction. In practice, the x direction is called the *sagittal axis*; the MR system acquires the 3D data cube by collecting several 2D scans along the sagittal axis by the process we described in the homework handout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59ZYUJy3Mq-"
      },
      "source": [
        "## Wavelet Sparsity of the Data\n",
        "\n",
        "Natural images, like the spatial domain sagittal MRI slices that our MR system acquires (post-Fourier-transformation), can often be represented concisely in a 2D Wavelet basis. We will see in later lectures/homeworks how this makes the techniques of compressed sensing applicable. For now, we will investigate empirically the transform-domain sparsity of a representative sagittal slice of our MR data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT4PhdEB5hJe"
      },
      "source": [
        "### Fixing our Slice\n",
        "\n",
        "We choose the sagittal slice we plotted above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te2PYVMf5gY6"
      },
      "source": [
        "X = data[Nx//2, :, :];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOjBmqqt4CwE"
      },
      "source": [
        "### Loading and Viewing Wavelet Basis Vectors\n",
        "\n",
        "We choose to represent our image in a db4 ([Daubechies](https://en.wikipedia.org/wiki/Ingrid_Daubechies) 4) wavelet basis, with a 2-level wavelet transformation (more on that below). In code, where we are interested in efficient computation of these transformations, we do not have direct access to the (inefficient-to-use) basis vectors themselves. But we can generate their spatial-domain representations by the pseudocode\n",
        "\n",
        "``` w_i = inverse_wavelet_transform(e_i) ```,\n",
        "\n",
        "where `w_i` represents the `i`-th wavelet basis function and `e_i` represents the `i`-th standard basis element. In this case, these standard basis elements are matrices with a 1 in the `i`-th position and zeros otherwise, where `i` runs from `1` up to `mn`; here our data are square, and `m = n = 256`. Be sure you understand the logic here---it is a good chance to refresh your understanding of linear algebra, if you need to do so!\n",
        "\n",
        "Below, we examine some of the wavelet basis functions. We use the implementation provided by [pywavelet](https://pywavelets.readthedocs.io/en/latest/). **Note**: it is essential here to set the ``mode = periodization`` wavelet transformation option in order to have the `dwt2` and `idwt2` functions behave (as-is) as linear operators on the space of matrices with dimensions equal to those of our input slice; we have made sure to do this in our pre-defined wrappers to these transforms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-2GcaMBQWds"
      },
      "source": [
        "## Examine a one-layer wavelet decomposition basis function.\n",
        "bpl.output_notebook()\n",
        "\n",
        "n = Nz\n",
        "levels = 2  # Change this to set the number of recursive wavelet transforms to use.\n",
        "\n",
        "I = 46\n",
        "J = 110  # Change these to view other basis elements. Must have 0 <= I, J <= n-1\n",
        "\n",
        "E = stdbel(n, I, J)\n",
        "F = idwt(levels, E)\n",
        "imagesc(F)\n",
        "print('Frobenius norm of F: {}'.format(np.linalg.norm(F, ord='fro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pStIXbpzRut_"
      },
      "source": [
        "Our check on the norm of `F` shows that the inverse transform has proper normalization here (unlike, for example, in the standard convention for FFT algorithms), so that the wavelet transform is indeed unitary.\n",
        "\n",
        "Be sure to zoom in to the part of the image above that is nonzero, to get a better sense of what the wavelet motif is. You may also try entering different indices above to view different basis elements. The wavelet basis functions have the interesting property of being very localized in time--their support is quite small. This makes them good at concisely representing edges in natural images.\n",
        "\n",
        "We will examine one other thing: the Fourier transform of the above wavelet basis element.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9X7BOA3S24c"
      },
      "source": [
        "## Examine the 2D Fourier magnitudes of the above wavelet basis element\n",
        "bpl.output_notebook()\n",
        "\n",
        "F_Fourier = np.fft.fft2(F, norm='ortho')\n",
        "imagesc(np.fft.fftshift(np.absolute(F_Fourier)),\n",
        "        title='Fourier magnitudes of wavelet basis vector')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g27FJ5wpT31W"
      },
      "source": [
        "Intriguingly, this basis element is also localized in frequency: points in the above figure that are black represent points where the 2D Fourier transform of the basis element is very close to zero, showing that it has (approximately) compact support! This property of being time-and-frequency-localized simultaneously is characteristic of wavelet basis elements.\n",
        "\n",
        "**Note**: Computational FFT methods return the Fourier transform on the square `[0, 1]^2`; recall that the 2D FFT is 1-periodic in each of its two normalized spatial variables independently. This implies that the low-frequency content of the signal is located in the \"corners\" of the raw Fourier transform magnitude estimate obtained; the plot above applies the `fftshift` function, which effectively transforms the domain to `[-0.5, +0.5]^2` and puts low frequencies in the center of the plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhlpLCxCUMnx"
      },
      "source": [
        "### Studying the Wavelet Transform of our MR Image\n",
        "\n",
        "Now we apply the wavelet transform we used above to our MR slice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inFLM9_03J4Q"
      },
      "source": [
        "bpl.output_notebook()\n",
        "W = dwt(levels, X)\n",
        "imagesc(W, '{}-level wavelet transform of MR image'.format(levels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwIdrIR9oPFV"
      },
      "source": [
        "The original image can be located in the bottom-left (by default) of the transformed result. Play around with the `levels` variable set above (set it to 1, 2, 3, maybe more...) to see the recursive nature of the discrete wavelet transform: a single level of the transform turns the input image into four new versions (LL, LH, HL, HH, from left-to-right and bottom-to-top) that contain information about the image at different resolutions, and further levels recurse on the LL level of the previous level. For more on this, see the description in the course notes.\n",
        "\n",
        "The wavelet transform is an orthonormal transform, and the above image suggests it indeed does a good job of sparsely representing the original MR image: lots of coefficients above appear to be zero! You will investigate this further below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sTIVnZ1pF-a"
      },
      "source": [
        "## Your Tasks\n",
        "\n",
        "Each level three header below contains a task you should complete. See the homework handout for additional details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qupowrI6pQaC"
      },
      "source": [
        "### Task 1: Top-`k` Reconstruction\n",
        "\n",
        "Plot the sorted magnitudes of the wavelet coefficients of the image `X`, as we generated them above, for 1, 2, and 3-level wavelet transformations. \n",
        "\n",
        "The output should be: the sorted wavelet coefficient magnitude figures for one, two, and three level wavelet transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the sorted magnitudes of the wavelet coefficients of the image `X`"
      ],
      "metadata": {
        "id": "llH6-VNzsDhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "coef = {}\n",
        "plt.figure(figsize=(8, 6), dpi = 100)\n",
        "for levels in [1,2,3]: \n",
        "  W = dwt(levels, X)\n",
        "  recons = W.copy()\n",
        "  #sorting the coefficients in the decreasing order\n",
        "  coef[levels] = np.sort(recons,axis = None)[::-1]\n",
        "  plt.plot(range(1,65537),coef[levels], label=levels)\n",
        "  plt.title(\"Coeffcient Magnitude in decreasing order for a given level\") \n",
        "  plt.xlabel(\"features\") \n",
        "  plt.ylabel(\"Coeffcient Magnitude\") \n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "GWSAylWr8kXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **From the above graph we can see that the decrease in the magnitudes of the wavelet coefficients for Level-3 (shown as green) is the highest which means that at Level-3, with the sparse assumption we can have the best image reconstruction as compared to the other 2 levels**"
      ],
      "metadata": {
        "id": "xSbayyTtseVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now choose a single level of transform to use based on which one appears to have the fastest coefficient magnitude decay, and use the following subsampling strategy to estimate the importance of the large-magnitude wavelet coefficients:\n",
        "1. Set all but the top `k` wavelet coefficients to be zero (essentially, discarding them), and reconstruct image by taking the inverse wavelet transform of the discarded coefficient matrix. \n",
        "2. Compute the approximation error as the ratio between the Frobenius norm of the difference between reconstructed and original image and the Frobenius norm of the original image, which should be $\\frac{\\Vert \\text{reconstructed} - \\text{original}\\Vert_F}{\\Vert \\text{original} \\Vert_F}$. \n",
        "3. Repeat this for at least 5 values of `k`, from `0` up to `256*256`; choose them based on the plots from the previous part, so that your results demonstrate the degradation of approximation error as you throw away more and more coefficients.\n",
        "4. plot the approximation error versus `k`, and estimate what would be the `k` that gives `0.05` approximation error. \n",
        "\n",
        "The outputs should be: \n",
        "1. figures showing the reconstructed images for each `k`\n",
        "2. a plot of the approximation error versus `k`. \n",
        "3. `k` that would give around `0.05` approximation error. \n",
        "\n",
        "**Hint**: You can find an example of this computation in the MRI demo posted on Courseworks. You still need to implement the above plots here in Colab using the machinery we have developed in this notebook, though."
      ],
      "metadata": {
        "id": "V3FPL3J6pzrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **For Level-3 (highest coefficient magnitude decrease) & for k = 350000 highest magnitude coefficients, we compute the reconstructed matrix by assigning 0 to the coefficients which are not in the top k highest magnitude coefficients**"
      ],
      "metadata": {
        "id": "Nrsy5bPesaak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing level-3 for computing the reconstructed matrix with k=35000 \n",
        "levels = 3\n",
        "total_coeff = X.shape[0] * X.shape[1]\n",
        "k = 35000\n",
        "W = dwt(levels, X)\n",
        "#choosing top k coefficents having the highest magnitude\n",
        "sorted_coeff = np.sort(recons,axis = None)[::-1][:k]\n",
        "recons = W.copy()\n",
        "#Assigning 0 to the coefficients which are not in the top k highest magnitude coefficients \n",
        "for i in range(recons.shape[0]):\n",
        "  for j in range(recons.shape[1]):\n",
        "    if recons[i][j] not in sorted_coeff:\n",
        "      recons[i][j] = 0"
      ],
      "metadata": {
        "id": "Yb8ISnwJIMNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Reconstructing image by taking the inverse wavelet transform of the discarded coefficient matrix**"
      ],
      "metadata": {
        "id": "y-TtpX6pvYmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpl.output_notebook()\n",
        "M = idwt(levels, recons)\n",
        "imagesc(M, title='Reconstructed Image for k = 35000 & Level = 3')"
      ],
      "metadata": {
        "id": "VOq5lyh_OqQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Computing the approximation error as the ratio between the Frobenius norm of the difference between reconstructed and original image and the Frobenius norm of the original image**"
      ],
      "metadata": {
        "id": "_fXuDd4bvrQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frob_norm_recons = np.linalg.norm(recons, ord='fro')\n",
        "frob_norm_original = np.linalg.norm(X, ord='fro')\n",
        "approximation_error = abs(frob_norm_recons - frob_norm_original) / abs(frob_norm_original)\n",
        "approximation_error"
      ],
      "metadata": {
        "id": "PGIgpBmOOQRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For 18 different values of k ranging from 10 to 65000, we plot the reconstructed image and compute the approximation error, to justify the decrease in the approximation error as more and more coefficient magnitudes are chosen and vice versa** "
      ],
      "metadata": {
        "id": "ROcg0NT-v1X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_coeff = X.shape[0] * X.shape[1]\n",
        "approx_error_arr = []\n",
        "k_arr = [10,300,500,800,1000,1200,1400,1600,1800,2000,5000,10000,20000,30000,40000,50000,60000,65000]\n",
        "for k in k_arr:\n",
        "  W = dwt(levels, X)\n",
        "  recons = W.copy()\n",
        "  sorted_coeff = np.sort(recons,axis = None)[::-1][:k]\n",
        "  for i in range(recons.shape[0]):\n",
        "    for j in range(recons.shape[1]):\n",
        "      if recons[i][j] not in sorted_coeff:\n",
        "        recons[i][j] = 0\n",
        "  frob_norm_recons = np.linalg.norm(recons, ord='fro')\n",
        "  frob_norm_original = np.linalg.norm(X, ord='fro')\n",
        "  approximation_error = abs(frob_norm_recons - frob_norm_original) / frob_norm_original\n",
        "  approx_error_arr.append(approximation_error)   \n",
        "  print(\"approximation_error for k = {} : {}\".format(k,approximation_error))\n",
        "  bpl.output_notebook()\n",
        "  M = idwt(levels, recons)\n",
        "  imagesc(M, title='Reconstructed image for k = {}'.format(k))"
      ],
      "metadata": {
        "id": "objqLHnmffRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the above figures, it is evident that there is degradation of approximation error as you throw away more and more coefficients."
      ],
      "metadata": {
        "id": "c4FpHaDRw4uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Figure shows the plot of approximation error vs k. There is decrease in the approximation error as k increases and more and more higher value coefficents are included**"
      ],
      "metadata": {
        "id": "V6t-L9uexVSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8, 6), dpi = 110)\n",
        "plt.plot(k_arr,approx_error_arr)\n",
        "plt.title(\"approximation error versus k\") \n",
        "plt.xlabel(\"k\") \n",
        "plt.ylabel(\"approximation error\") "
      ],
      "metadata": {
        "id": "fJJJV-0eqOkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min,k = 1,0\n",
        "for req_k in range(total_coeff):\n",
        "  app_error = np.interp(req_k,k_arr,approx_error_arr)\n",
        "  if abs(0.05 - app_error) < min:\n",
        "    min = abs(0.05 - app_error)\n",
        "    k = req_k\n",
        "print(\" Value of k that gives 0.05 approximation error : {} \".format(k))  \n"
      ],
      "metadata": {
        "id": "Zw_uxfwr6g37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **From the above code, Value of k that gives 0.05 approximation error is computed as : 1184**"
      ],
      "metadata": {
        "id": "YNlxmuxfxvED"
      }
    }
  ]
}